{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**What?** Prediction intervals with gradient boosting\n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theortical recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **A CONFIDENCE interval** quantifies the uncertainty on an estimated population variable, such as the mean or \n",
    "standard deviation. It can be used to quantify the uncertainty of the estimated skill of a model.\n",
    "- **A PREDICTION interval** quantifies the uncertainty on a single observation estimated from the population. It\n",
    "can be used to quantify the uncertainty of a single forecast. \n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project's goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Prediction building energy consumption and the associated prediction interval for each prediction. \n",
    "- There are undoubtedly hidden features (latent variables) not captured in our data that affect energy consumption.\n",
    "- **THEREFORE**, we want to show the uncertainty in our estimates by predicting both an upper and lower bound for energy use.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import glob\n",
    "from ipywidgets import interact, widgets\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot, plot, init_notebook_mode\n",
    "init_notebook_mode(connected=True)\n",
    "import plotly_express as px\n",
    "import cufflinks as cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check packages version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy,pandas,plotly\n",
    "%load_ext watermark\n",
    "%watermark -p numpy,pandas,plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The energy data is measured every 15 minutes and includes 3 weather variables related to energy consumption: temperature, irradiance, and relative humidity. \n",
    "- This is the data from the DrivenData Energy Forecasting competition.\n",
    "- I've cleaned up the datasets and extracted 8 features that allow us to predict the energy consumption fairly accurately.\n",
    "- Dataset refrence: https://www.drivendata.org/competitions/51/electricity-prediction-machine-learning/\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('../DATASETS/*_energy_data.csv')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(files[2], parse_dates=['timestamp'],\n",
    "                   index_col='timestamp').sort_index()\n",
    "data.head()\n",
    "data = data.rename(columns={\"energy\": \"actual\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of data for plotting\n",
    "data_to_plot = data.loc[\"2015\"].copy()\n",
    "\n",
    "\n",
    "def plot_timescale(timescale, selection, theme):\n",
    "    \"\"\"\n",
    "    Plot the energy consumption on different timescales (day, week, month).\n",
    "    \n",
    "    :param timescale: the timescale to use\n",
    "    :param selection: the numeric value of the timescale selection (for example the 15th day\n",
    "    of the year or the 1st week of the year)\n",
    "    :param theme: aesthetics of plot\n",
    "    \"\"\"\n",
    "    # Subset based on timescale and selection\n",
    "    subset = data_to_plot.loc[\n",
    "        getattr(data_to_plot.index, timescale) == selection, \"actual\"\n",
    "    ].copy()\n",
    "\n",
    "    if subset.empty:\n",
    "        print(\"Choose another selection\")\n",
    "        return\n",
    "    \n",
    "    # Make an interactive plot\n",
    "    fig = subset.iplot(\n",
    "            title=f\"Energy for {selection} {timescale.title()}\", theme=theme, asFigure=True\n",
    "    )\n",
    "    fig['layout']['height'] = 500\n",
    "    fig['layout']['width'] = 1400\n",
    "    iplot(fig)\n",
    "    \n",
    "\n",
    "\n",
    "_ = interact(\n",
    "    plot_timescale,\n",
    "    timescale=widgets.RadioButtons(\n",
    "        options=[\"dayofyear\", \"week\", \"month\"], value=\"dayofyear\"\n",
    "    ),\n",
    "    # Selection \n",
    "    selection=widgets.IntSlider(value=16, min=0, max=365),\n",
    "    theme=widgets.Select(options=cf.themes.THEMES.keys(), value='ggplot')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.loc['2015-01-01':'2015-07-01', \"actual\"].iplot(layout=dict(title='2015 Energy Consumption', height=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Clearly, there are different patterns in energy usage over the course of a day, week, and month. \n",
    "- We can also look at longer timescales. \n",
    "- Plotting this much data can make the notebook slow. Instead, we can resample the data and plot to see any long term trends.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.resample('12 H')[\"actual\"].mean().iplot(layout=dict(title='Energy Data Resampled at 12 Hours', height=500,\n",
    "                                                        yaxis=dict(title='kWh')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Predicting Intervals with the Gradient Boosting Regressor. GB builds an additive model in a forward stage-wise \n",
    "fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree\n",
    "is fit on the negative gradient of the given loss function. From sklearn userguide we know these are the options:\n",
    "loss{‘ls’, ‘lad’, ‘huber’, ‘quantile’}.\n",
    "\n",
    "    - For the lower prediction, use the GradientBoostingRegressor with loss='quantile' and alpha=lower_quantile \n",
    "      (for example, 0.1 for the 10th percentile)\n",
    "    - For the upper prediction, use the GradientBoostingRegressor with loss='quantile' and alpha=upper_quantile \n",
    "      (for example, 0.9 for the 90th percentile)\n",
    "    - For the mid prediction, use GradientBoostingRegressor(loss=\"quantile\", alpha=0.5) which predicts the median,\n",
    "      or the default loss=\"ls\" (for least squares) which predicts the mean which was we are going to use \n",
    "      \n",
    "- **IMPORTANT POINT:** when we change the loss to quantile and choose alpha (the quantile), we’re able to get predictions \n",
    "corresponding to percentiles. If we use lower and upper quantiles, we can produce an estimated range which is exactly \n",
    "what we want.   \n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test sets\n",
    "X_train = data.loc[\"2015\":\"2016\"].copy()\n",
    "X_test = data.loc[\"2017\":].copy()\n",
    "y_train = X_train.pop(\"actual\")\n",
    "y_test = X_test.pop(\"actual\")\n",
    "\n",
    "assert X_train.index.max() < X_test.index.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set lower and upper quantile\n",
    "LOWER_ALPHA = 0.15\n",
    "UPPER_ALPHA = 0.85\n",
    "\n",
    "N_ESTIMATORS = 100\n",
    "MAX_DEPTH = 5\n",
    "\n",
    "\n",
    "lower_model = GradientBoostingRegressor(loss = \"quantile\", alpha=LOWER_ALPHA, n_estimators=N_ESTIMATORS, \n",
    "                                        max_depth=MAX_DEPTH)\n",
    "\n",
    "# The mid model will use the default ls which predict the mean\n",
    "mid_model = GradientBoostingRegressor(loss = \"ls\", n_estimators=N_ESTIMATORS, max_depth=MAX_DEPTH)\n",
    "\n",
    "upper_model = GradientBoostingRegressor(loss = \"quantile\", alpha=UPPER_ALPHA, n_estimators=N_ESTIMATORS, \n",
    "                                        max_depth=MAX_DEPTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Build Models for Lower, Upper Quantile and Mean. Remember our goal is to get the interval, at the moment we are \n",
    "not concentrated on the getting the best, hence there is no hyperparameters tuning here.\n",
    "- The models are trained based on optimizing for the specific loss function. \n",
    "- This means we have to build 3 separate models to predict the different objectives. \n",
    "- A downside of this method is that it's a little slow, particularly because we can't parallelize training on the Scikit-Learn Gradient Boosting Regresssor. \n",
    "- If you wanted, you could re-write this code to train each model on a separate processor (using multiprocessing.)\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = lower_model.fit(X_train, y_train)\n",
    "_ = mid_model.fit(X_train, y_train)\n",
    "_ = upper_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-  With the models all trained, we now make predictions and record them with the true values.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(y_test)\n",
    "predictions['lower'] = lower_model.predict(X_test)\n",
    "predictions['mid'] = mid_model.predict(X_test)\n",
    "predictions['upper'] = upper_model.predict(X_test)\n",
    "\n",
    "assert (predictions['upper'] > predictions['lower']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Intervals Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_intervals(predictions, mid=False, start=None, stop=None, title=None):\n",
    "    \"\"\"\n",
    "    Function for plotting prediction intervals as filled area chart.\n",
    "    \n",
    "    :param predictions: dataframe of predictions with lower, upper, and actual columns (named for the target)\n",
    "    :param whether to show the mid prediction\n",
    "    :param start: optional parameter for subsetting start of predictions\n",
    "    :param stop: optional parameter for subsetting end of predictions\n",
    "    :param title: optional string title\n",
    "    \n",
    "    :return fig: plotly figure\n",
    "    \"\"\"\n",
    "    # Subset if required\n",
    "    predictions = (\n",
    "        predictions.loc[start:stop].copy()\n",
    "        if start is not None or stop is not None\n",
    "        else predictions.copy()\n",
    "    )\n",
    "    data = []\n",
    "\n",
    "    # Lower trace will fill to the upper trace\n",
    "    trace_low = go.Scatter(\n",
    "        x=predictions.index,\n",
    "        y=predictions[\"lower\"],\n",
    "        fill=\"tonexty\",\n",
    "        line=dict(color=\"darkblue\"),\n",
    "        fillcolor=\"rgba(173, 216, 230, 0.4)\",\n",
    "        showlegend=True,\n",
    "        name=\"lower\",\n",
    "    )\n",
    "    # Upper trace has no fill\n",
    "    trace_high = go.Scatter(\n",
    "        x=predictions.index,\n",
    "        y=predictions[\"upper\"],\n",
    "        fill=None,\n",
    "        line=dict(color=\"orange\"),\n",
    "        showlegend=True,\n",
    "        name=\"upper\",\n",
    "    )\n",
    "\n",
    "    # Must append high trace first so low trace fills to the high trace\n",
    "    data.append(trace_high)\n",
    "    data.append(trace_low)\n",
    "    \n",
    "    if mid:\n",
    "        trace_mid = go.Scatter(\n",
    "        x=predictions.index,\n",
    "        y=predictions[\"mid\"],\n",
    "        fill=None,\n",
    "        line=dict(color=\"green\"),\n",
    "        showlegend=True,\n",
    "        name=\"mid\",\n",
    "    )\n",
    "        data.append(trace_mid)\n",
    "\n",
    "    # Trace of actual values\n",
    "    trace_actual = go.Scatter(\n",
    "        x=predictions.index,\n",
    "        y=predictions[\"actual\"],\n",
    "        fill=None,\n",
    "        line=dict(color=\"black\"),\n",
    "        showlegend=True,\n",
    "        name=\"actual\",\n",
    "    )\n",
    "    data.append(trace_actual)\n",
    "\n",
    "    # Layout with some customization\n",
    "    layout = go.Layout(\n",
    "        height=500,\n",
    "        width=1400,\n",
    "        title=dict(text=\"Prediction Intervals\" if title is None else title),\n",
    "        yaxis=dict(title=dict(text=\"kWh\")),\n",
    "        xaxis=dict(\n",
    "            rangeselector=dict(\n",
    "                buttons=list(\n",
    "                    [\n",
    "                        dict(count=1, label=\"1d\", step=\"day\", stepmode=\"backward\"),\n",
    "                        dict(count=7, label=\"1w\", step=\"day\", stepmode=\"backward\"),\n",
    "                        dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n",
    "                        dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n",
    "                        dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n",
    "                        dict(step=\"all\"),\n",
    "                    ]\n",
    "                )\n",
    "            ),\n",
    "            rangeslider=dict(visible=True),\n",
    "            type=\"date\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "    # Make sure font is readable\n",
    "    fig[\"layout\"][\"font\"] = dict(size=20)\n",
    "    fig.layout.template = \"plotly_white\"\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Example plot subsetted to one week\n",
    "fig = plot_intervals(predictions, start=\"2017-03-01\", stop=\"2017-03-08\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive plotting\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Quantifying the error of a prediction range can be tricky. We'll start off with the percentage of the time that the \n",
    "actual value falls in the range. \n",
    "- However, one way to maximize this metric would be to just use extremely wide prediction intervals. \n",
    "- Therefore, we want to penalize the model for making too wide prediction intervals. \n",
    "- As a simple example we can calculate the absolute error of the bottom and top lines, and divide by two to get an absolute error. We then take the average for the mean absolute error. \n",
    "- We can also calculate the absolute error of the mid predictions.\n",
    "- These are likely not the best metrics for all cases.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(predictions):\n",
    "    \"\"\"\n",
    "    Calculate the absolute error associated with prediction intervals\n",
    "    \n",
    "    :param predictions: dataframe of predictions\n",
    "    :return: None, modifies the prediction dataframe    \n",
    "    \"\"\"\n",
    "    \n",
    "    predictions['absolute_error_lower'] = (predictions['lower'] - predictions[\"actual\"]).abs()\n",
    "    predictions['absolute_error_upper'] = (predictions['upper'] - predictions[\"actual\"]).abs()\n",
    "    \n",
    "    predictions['absolute_error_interval'] = (predictions['absolute_error_lower'] + predictions['absolute_error_upper']) / 2\n",
    "    predictions['absolute_error_mid'] = (predictions['mid'] - predictions[\"actual\"]).abs()\n",
    "    \n",
    "    predictions['in_bounds'] = predictions[\"actual\"].between(left=predictions['lower'], right=predictions['upper'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_error(predictions)\n",
    "metrics = predictions[['absolute_error_lower', 'absolute_error_upper', 'absolute_error_interval', 'absolute_error_mid', 'in_bounds']].copy()\n",
    "metrics.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- We see the lower prediction has a smaller absolute error (in terms of the median). \n",
    "- It's interesting the absolute error for the lower bound is actually less than that for the middle prediction! \n",
    "- We can write a short function to display the metrics.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics(metrics):\n",
    "    \"\"\"\n",
    "    Make a boxplot of the metrics associated with prediction intervals\n",
    "    \n",
    "    :param metrics: dataframe of metrics produced from calculate error \n",
    "    :return fig: plotly figure\n",
    "    \"\"\"\n",
    "    percent_in_bounds = metrics['in_bounds'].mean() * 100\n",
    "    metrics_to_plot = metrics[[c for c in metrics if 'absolute_error' in c]]\n",
    "\n",
    "    # Rename the columns\n",
    "    metrics_to_plot.columns = [column.split('_')[-1].title() for column in metrics_to_plot]\n",
    "\n",
    "    # Create a boxplot of the metrics\n",
    "    fig = px.box(\n",
    "        metrics_to_plot.melt(var_name=\"metric\", value_name='Absolute Error'),\n",
    "        x=\"metric\",\n",
    "        y=\"Absolute Error\",\n",
    "        color='metric',\n",
    "        title=f\"Error Metrics Boxplots    In Bounds = {percent_in_bounds:.2f}%\",\n",
    "        height=500,\n",
    "        width=1000,\n",
    "        points=False,\n",
    "    )\n",
    "\n",
    "    # Create new data with no legends\n",
    "    d = []\n",
    "\n",
    "    for trace in fig.data:\n",
    "        # Remove legend for each trace\n",
    "        trace['showlegend'] = False\n",
    "        d.append(trace)\n",
    "\n",
    "    # Make the plot look a little better\n",
    "    fig.data = d\n",
    "    fig['layout']['font'] = dict(size=20)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iplot(show_metrics(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example plot subsetted to one WEEK -> YEAR-MONTH-DAY\n",
    "fig = plot_intervals(predictions, mid=True, start=\"2017-03-01\", stop=\"2017-03-08\")\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a class for the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- To make this process repeatable, we can build our own estimator with a Scikit-Learn interface that fits and predicts all 3 models in one call each. \n",
    "- This is a very simple class but can be extended based on your needs.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingPredictionIntervals(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Model that produces prediction intervals with a Scikit-Learn inteface\n",
    "    \n",
    "    :param lower_alpha: lower quantile for prediction, default=0.1\n",
    "    :param upper_alpha: upper quantile for prediction, default=0.9\n",
    "    :param **kwargs: additional keyword arguments for creating a GradientBoostingRegressor model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lower_alpha=0.1, upper_alpha=0.9, **kwargs):\n",
    "        self.lower_alpha = lower_alpha\n",
    "        self.upper_alpha = upper_alpha\n",
    "\n",
    "        # Three separate models\n",
    "        self.lower_model = GradientBoostingRegressor(\n",
    "            loss=\"quantile\", alpha=self.lower_alpha, **kwargs)\n",
    "        \n",
    "        self.mid_model = GradientBoostingRegressor(loss=\"ls\", **kwargs)\n",
    "        \n",
    "        self.upper_model = GradientBoostingRegressor(\n",
    "            loss=\"quantile\", alpha=self.upper_alpha, **kwargs)\n",
    "        self.predictions = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit all three models\n",
    "            \n",
    "        :param X: train features\n",
    "        :param y: train targets\n",
    "        \n",
    "        TODO: parallelize this code across processors\n",
    "        \"\"\"\n",
    "        self.lower_model.fit(X_train, y_train)\n",
    "        self.mid_model.fit(X_train, y_train)\n",
    "        self.upper_model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X, y):\n",
    "        \"\"\"\n",
    "        Predict with all 3 models \n",
    "        \n",
    "        :param X: test features\n",
    "        :param y: test targets\n",
    "        :return predictions: dataframe of predictions\n",
    "        \n",
    "        TODO: parallelize this code across processors\n",
    "        \"\"\"\n",
    "        predictions = pd.DataFrame(y)\n",
    "        predictions[\"lower\"] = self.lower_model.predict(X)\n",
    "        predictions[\"mid\"] = self.mid_model.predict(X)\n",
    "        predictions[\"upper\"] = self.upper_model.predict(X)\n",
    "        self.predictions = predictions\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def plot_intervals(self, mid=False, start=None, stop=None):\n",
    "        \"\"\"\n",
    "        Plot the prediction intervals\n",
    "        \n",
    "        :param mid: boolean for whether to show the mid prediction\n",
    "        :param start: optional parameter for subsetting start of predictions\n",
    "        :param stop: optional parameter for subsetting end of predictions\n",
    "    \n",
    "        :return fig: plotly figure\n",
    "        \"\"\"\n",
    "\n",
    "        if self.predictions is None:\n",
    "            raise ValueError(\"This model has not yet made predictions.\")\n",
    "            return\n",
    "        \n",
    "        fig = plot_intervals(predictions, mid=mid, start=start, stop=stop)\n",
    "        return fig\n",
    "    \n",
    "    def calculate_and_show_errors(self):\n",
    "        \"\"\"\n",
    "        Calculate and display the errors associated with a set of prediction intervals\n",
    "        \n",
    "        :return fig: plotly boxplot of absolute error metrics\n",
    "        \"\"\"\n",
    "        if self.predictions is None:\n",
    "            raise ValueError(\"This model has not yet made predictions.\")\n",
    "            return\n",
    "        \n",
    "        calculate_error(self.predictions)\n",
    "        fig = show_metrics(self.predictions)\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingPredictionIntervals(lower_alpha=0.1, upper_alpha=0.9, n_estimators=50, max_depth=3)\n",
    "\n",
    "# Fit and make predictions\n",
    "_ = model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_fig = model.calculate_and_show_errors()\n",
    "iplot(metric_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = model.plot_intervals(mid=True, start='2017-05-26', \n",
    "                           stop='2017-06-01')\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantile loss explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The quantile loss for a predicted is expressed as:\n",
    "- quantile loss = α∗(actual−predicted)if(actual−predicted)>0\n",
    "- quantile loss = (α−1)∗(actual−predicted)if(actual−predicted)<0\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_quantile_loss(quantile, actual, predicted):\n",
    "    \"\"\"\n",
    "    Quantile loss for a given quantile and prediction\n",
    "    \"\"\"\n",
    "    return np.maximum(quantile * (actual - predicted), (quantile - 1) * (actual - predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- When we graph the quantile loss versus the error, the weighting of the errors appears as the slope.\n",
    "- Let's walk through an example using lower quantile = 0.1, upper quantile = 0.9, and actual value = 10. \n",
    "- There are four possibilities for the predictions:\n",
    "\n",
    "    - Prediction = 15 with Quantile = 0.1. Actual < Predicted; Loss = (0.1 - 1) * (10 - 15) = 4.5\n",
    "    - Prediction = 5 with Quantile = 0.1. Actual > Predicted; Loss = 0.1 * (10 - 5) = 0.5\n",
    "    - Predicted = 15 with Quantile = 0.9. Actual < Predicted; Loss = (0.9 - 1) * (10 - 15) = 0.5\n",
    "    - Predicted = 5 with Quantile = 0.9. Actual < Predicted; Loss = 0.9 * (10 - 5) = 4.5\n",
    "\n",
    "- For cases where the quantile > 0.5 we penalize low predictions more heavily. For cases where the quantile < 0.5 we \n",
    "penalize high predictions more heavily.\n",
    "- If the quantile = 0.5, then the weighting is the same for both low and high predictions. For quantile == 0.5, we\n",
    "are predicting the median.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_quantile_loss(actual, prediction_list, quantile_list, plot_ls=False):\n",
    "    \"\"\"\n",
    "    Shows the quantile loss associated with predictions at different quantiles.\n",
    "    Figure shows the loss versus the error\n",
    "    \n",
    "    :param actual: array-like of actual values\n",
    "    :param prediction_list: list of array-like predictions\n",
    "    :param quantile_list: list of float quantiles corresponding to the predictions\n",
    "    :param plot_ls: whether to plot the least squares loss\n",
    "    \n",
    "    :return fig: plotly figure\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    # Iterate through each combination of prediction and quantile\n",
    "    for predictions, quantile in zip(prediction_list, quantile_list):\n",
    "        # Calculate the loss\n",
    "        quantile_loss = calculate_quantile_loss(quantile, actual, predictions)\n",
    "        \n",
    "        errors = actual - predictions\n",
    "        # Sort errors and loss by error\n",
    "        idx = np.argsort(errors)\n",
    "        errors = errors[idx]; quantile_loss = quantile_loss[idx]\n",
    "    \n",
    "        # Add data to plot\n",
    "        data.append(go.Scatter(mode=\"lines\", x=errors, y=quantile_loss, line=dict(width=4), name=f\"{quantile} Quantile\"))\n",
    "        \n",
    "    if plot_ls:\n",
    "        loss = np.square(predictions - actual)\n",
    "        errors = actual - predictions\n",
    "        \n",
    "        # Sort errors and loss by error\n",
    "        idx = np.argsort(errors)\n",
    "        errors = errors[idx]; loss = loss[idx]\n",
    "    \n",
    "        # Add data to plot\n",
    "        data.append(go.Scatter(mode=\"lines\", x=errors, y=loss, line=dict(width=4), name=\"Least Squares\"))\n",
    "        \n",
    "    # Simple plot layout\n",
    "    layout = go.Layout(\n",
    "        title=\"Quantile Loss vs Error\",\n",
    "        yaxis=dict(title=\"Loss\"),\n",
    "        xaxis=dict(title=\"Error\"),\n",
    "        width=1000, height=500,\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    fig['layout']['font'] = dict(size=18)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dummy predictions and actual values\n",
    "predictions = np.arange(-2.1, 2.1, step=0.1)\n",
    "actual = np.zeros(len(predictions))\n",
    "\n",
    "# Create a plot showing the same predictions at different quantiles\n",
    "fig = plot_quantile_loss(actual, [predictions, predictions, predictions], [0.1, 0.5, 0.9], False)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- We can see how the quantile loss is asymmetric with a weighting (slope) equal to the quantile value or to \n",
    "(quantile - 1) depending on if the error is positive or negative. \n",
    "- With an error defined as (actual - predicted), for a quantile greater than 0.5, we penalize positive errors more and for a quantile less than 0.5, we penalize negative errors more. \n",
    "- This drives the predictions with a higher quantile higher than the actual value, and predictions with a lower quantile lower than the actual value. The quantile loss is always positive.\n",
    "- This is a great reminder that the loss function of a machine learning method dictates what you are optimizing for!\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot with least squares loss as well\n",
    "fig = plot_quantile_loss(actual, [predictions, predictions, predictions], [0.1, 0.5, 0.9], True)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- With the default loss function — least squares — the gradient boosting regressor is predicting the mean. \n",
    "- The critical point to understand is that the least squares loss penalizes low and high errors equally. \n",
    "- In contrast, the quantile loss penalizes errors based on the quantile and whether the error was positive (actual > predicted) or negative (actual < predicted). \n",
    "- This allows the gradient boosting model to optimize not for the mean, but for percentiles.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predictions.copy()\n",
    "\n",
    "fig = plot_quantile_loss(\n",
    "    predictions[\"actual\"],\n",
    "    [predictions[\"lower\"], predictions[\"mid\"], predictions[\"upper\"]],\n",
    "    [model.lower_alpha, 0.5, model.upper_alpha],\n",
    ")\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- We can see the same weighting applied to the model's predictions. \n",
    "- When the error is negative - meaning the actual value was less than the predicted value - and the quantile is less than 0.5, we weight the error by (quantile - 1)to penalize the high prediction. \n",
    "- When the error is positive - meaning the actual value was greater than the predicted value - and the quantile is greater than 0.5, we weight the error by the quantile to penalize the low prediction.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- https://towardsdatascience.com/how-to-generate-prediction-intervals-with-scikit-learn-and-python-ab3899f992ed<br>\n",
    "- https://nbviewer.jupyter.org/github/WillKoehrsen/Data-Analysis/blob/master/prediction-intervals/prediction_intervals.ipynb<br>\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
